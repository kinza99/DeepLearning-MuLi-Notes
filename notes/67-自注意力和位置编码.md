# 自注意力和位置编码

  - [自注意力](#自注意力)
  - [比较卷积神经网络、循环神经网络和自注意力](#比较卷积神经网络循环神经网络和自注意力)
  - [位置编码](#位置编码)
  - [总结](#总结)



### 自注意力
- 有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出 由于查询、键和值来自同一组输入，因此被称为 *自注意力*。
- 给定任意的序列**𝐱**1,…,**𝐱**𝑛， 其中任意**𝐱**𝑖∈ℝ𝑑xi∈Rd（1≤𝑖≤𝑛1≤i≤n），自注意力使得**𝐱**𝑖自己当作key，query，value向量来得到 **𝐲**1,…,**𝐲**𝑛![截屏2022-02-16 下午2.18.20](/Users/duhe/Documents/67-01.png)

## 比较卷积神经网络、循环神经网络和自注意力

- 让我们比较下面几个架构，目标都是将由𝑛n个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由𝑑d维向量表示。具体来说，我们将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。![截屏2022-02-16 下午2.19.33](/Users/duhe/Documents/67-02.png)卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。

## 位置编码

- 自注意力的另一个区别于CNN \ RNN的特点是其没有记录位置信息，位置编码的作用是把位置信息注入到输入里。令位置编码为P，P的维度与输入X维度相同，最终把X+P作为最终输入。P的计算方式如下：
  $$
  p_{i,2j}=sin(\frac{i}{10000^{2j/d}}),\\p_{i,2j+1}=cos(\frac{i}{10000^{2j/d}}).
  $$

- 绝对位置信息和相对位置信息：

  - 我们将上述的位置编码方式与二进制编码做对比，在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。 这是因为对于任何确定的位置偏移𝛿，位置𝑖+𝛿处 的位置编码可以线性投影位置𝑖i处的位置编码来表示。

### 总结

- 在自注意力中，查询、键和值都来自同一组输入。
- 卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。
- 为了使用序列的顺序信息，我们可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。



